{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program-1 for web crawling\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pages_crawled = []\n",
    "\n",
    "\n",
    "def crawler(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    for link in links:\n",
    "        if 'href' in link.attrs:\n",
    "            if link['href'].startswith('/wiki') and ':' not in link['href']:\n",
    "                if link['href'] not in pages_crawled:\n",
    "                    new_link = f\"https://en.wikipedia.org{link['href']}\"\n",
    "                    pages_crawled.append(link['href'])\n",
    "                    try:\n",
    "                        with open('data.csv', 'a') as file:\n",
    "                            file.write(f'{soup.title.text}; {soup.h1.text}; {link[\"href\"]}\\n')\n",
    "                        crawler(new_link)\n",
    "                    except:\n",
    "                        continue\n",
    "                                       \n",
    "                                                                 \n",
    "crawler('https://en.wikipedia.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d87d3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program-2 for web crawling\n",
    "# Python3 program for a word frequency\n",
    "# counter after crawling/scraping a web-page\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import operator\n",
    "from collections import Counter\n",
    " \n",
    "'''Function defining the web-crawler/core\n",
    "spider, which will fetch information from\n",
    "a given website, and push the contents to\n",
    "the second  function clean_wordlist()'''\n",
    " \n",
    "def start(url):\n",
    " \n",
    "    # empty list to store the contents of\n",
    "    # the website fetched from our web-crawler\n",
    "    wordlist = []\n",
    "    source_code = requests.get(url).text\n",
    " \n",
    "    # BeautifulSoup object which will\n",
    "    # ping the requested url for data\n",
    "    soup = BeautifulSoup(source_code, 'html.parser')\n",
    " \n",
    "    # Text in given web-page is stored under\n",
    "    # the <div> tags with class <entry-content>\n",
    "    for each_text in soup.findAll('div', {'class': 'entry-content'}):\n",
    "        content = each_text.text\n",
    " \n",
    "        # use split() to break the sentence into\n",
    "        # words and convert them into lowercase\n",
    "        words = content.lower().split()\n",
    " \n",
    "        for each_word in words:\n",
    "            wordlist.append(each_word)\n",
    "        clean_wordlist(wordlist)\n",
    " \n",
    "# Function removes any unwanted symbols\n",
    " \n",
    "def clean_wordlist(wordlist):\n",
    " \n",
    "    clean_list = []\n",
    "    for word in wordlist:\n",
    "        symbols = \"!@#$%^&*()_-+={[}]|\\;:\\\"<>?/., \"\n",
    " \n",
    "        for i in range(len(symbols)):\n",
    "            word = word.replace(symbols[i], '')\n",
    " \n",
    "        if len(word) > 0:\n",
    "            clean_list.append(word)\n",
    "    create_dictionary(clean_list)\n",
    " \n",
    "# Creates a dictionary containing each word's\n",
    "# count and top_20 occurring words\n",
    "def create_dictionary(clean_list):\n",
    "    word_count = {}\n",
    " \n",
    "    for word in clean_list:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    " \n",
    "    ''' To get the count of each word in\n",
    "        the crawled page -->\n",
    " \n",
    "    # operator.itemgetter() takes one\n",
    "    # parameter either 1(denotes keys)\n",
    "    # or 0 (denotes corresponding values)\n",
    " \n",
    "    for key, value in sorted(word_count.items(),\n",
    "                    key = operator.itemgetter(1)):\n",
    "        print (\"% s : % s \" % (key, value))\n",
    " \n",
    "    <-- '''\n",
    " \n",
    "    c = Counter(word_count)\n",
    " \n",
    "    # returns the most occurring elements\n",
    "    top = c.most_common(10)\n",
    "    print(top)\n",
    " \n",
    " \n",
    "# Driver code\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://www.geeksforgeeks.org/programming-language-choose/\"\n",
    "    # starts crawling and prints output\n",
    "    start(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f793de6",
   "metadata": {},
   "source": [
    "#Web crawling\n",
    "\"\"\"\n",
    "A web crawler, crawler or web spider, is a computer program that's used to search and automatically \n",
    "index website content and other information over the internet. \n",
    "These programs, or bots, are most commonly used to create entries for a search engine index.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed6404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 /reset-client \n",
      " Movie: Trouble logging in?\n",
      "Movie info: N/A\n",
      "2 https://www.fandango.com/policies/privacy-policy \n",
      " Movie: Privacy Policy\n",
      "Movie info: N/A\n",
      "3 https://www.fandango.com/policies/terms-and-policies \n",
      " Movie: Terms and Policies\n",
      "Movie info: N/A\n",
      "4 https://www.fandango.com/policies/privacy-policy \n",
      " Movie: Privacy Policy\n",
      "Movie info: N/A\n",
      "5 https://www.fandango.com/policies/terms-and-policies \n",
      " Movie: Terms and Policies\n",
      "Movie info: N/A\n",
      "6 /help_desk \n",
      " Movie: HELP\n",
      "Movie info: N/A\n",
      "7 #main-page-content \n",
      " Movie: Skip to Main Content\n",
      "Movie info: N/A\n",
      "8 / \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "9 /search \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "10 / \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "11 /about#whatisthetomatometer \n",
      " Movie: What's the TomatometerÂ®?\n",
      "Movie info: N/A\n",
      "12 /critics \n",
      " Movie: Critics\n",
      "Movie info: N/A\n",
      "13 /user/account \n",
      " Movie: Account\n",
      "Movie info: N/A\n",
      "14 #logout \n",
      " Movie: Log Out\n",
      "Movie info: N/A\n",
      "15 /browse/movies_in_theaters \n",
      " Movie: Movies\n",
      "Movie info: N/A\n",
      "16 /browse/movies_in_theaters \n",
      " Movie: Movies in theaters\n",
      "Movie info: N/A\n",
      "17 /browse/movies_in_theaters/sort:newest \n",
      " Movie: Opening this week\n",
      "Movie info: N/A\n",
      "18 /browse/movies_coming_soon/ \n",
      " Movie: Coming soon to theaters\n",
      "Movie info: N/A\n",
      "19 /browse/movies_in_theaters/critics:certified_fresh~sort:popular \n",
      " Movie: Certified fresh movies\n",
      "Movie info: N/A\n",
      "20 /browse/movies_at_home \n",
      " Movie: Movies at home\n",
      "Movie info: N/A\n",
      "21 /browse/movies_at_home/affiliates:peacock \n",
      " Movie: Peacock\n",
      "Movie info: N/A\n",
      "22 /browse/movies_at_home/affiliates:vudu \n",
      " Movie: Vudu\n",
      "Movie info: N/A\n",
      "23 /browse/movies_at_home/affiliates:netflix \n",
      " Movie: Netflix streaming\n",
      "Movie info: N/A\n",
      "24 /browse/movies_at_home/affiliates:apple_tv_us,apple_tv_plus \n",
      " Movie: Apple TV\n",
      "Movie info: N/A\n",
      "25 /browse/movies_at_home/affiliates:amazon_prime \n",
      " Movie: Amazon prime\n",
      "Movie info: N/A\n",
      "26 /browse/movies_at_home/sort:popular \n",
      " Movie: Most popular streaming movies\n",
      "Movie info: N/A\n",
      "27 /browse/movies_at_home/critics:certified_fresh \n",
      " Movie: Certified fresh movies\n",
      "Movie info: N/A\n",
      "28 /browse/movies_at_home \n",
      " Movie: Browse all\n",
      "Movie info: N/A\n",
      "29 https://editorial.rottentomatoes.com/what-to-watch/  \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "30 /top \n",
      " Movie: Top movies\n",
      "Movie info: N/A\n",
      "31 /trailers \n",
      " Movie: Trailers\n",
      "Movie info: N/A\n",
      "32 /m/blue_beetle \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "33 /m/bottoms \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "34 /m/red_white_and_royal_blue \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "35 /browse/tv_series_browse/sort:popular \n",
      " Movie: Tv shows\n",
      "Movie info: N/A\n",
      "36 /tv/the_wheel_of_time/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "37 /tv/one_piece_2023/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "38 /tv/power_book_iv_force/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "39 /tv/spellbound/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "40 /tv/the_pact/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "41 /tv/the_conversations_project/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "42 /tv/archer/s14 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "43 /tv/disenchantment/s05 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "44 /tv/adventure_time_fionna_and_cake/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "45 /browse/tv_series_browse/sort:newest \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "46 /tv/who_is_erin_carter/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "47 /tv/star_wars_ahsoka/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "48 /tv/invasion_2021/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "49 /tv/shelter/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "50 /tv/ragnarok/s03 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "51 /tv/the_bear/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "52 /tv/foundation/s02 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "53 /tv/only_murders_in_the_building/s03 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "54 /tv/the_chosen_one_2023/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "55 /browse/tv_series_browse/sort:popular? \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "56 https://editorial.rottentomatoes.com/what-to-watch/  \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "57 /browse/tv_series_browse/sort:popular \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "58 /browse/tv_series_browse/critics:fresh \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "59 /browse/tv_series_browse/affiliates:peacock \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "60 /browse/tv_series_browse/affiliates:vudu \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "61 /browse/tv_series_browse/affiliates:netflix \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "62 /browse/tv_series_browse/affiliates:apple_tv_us,apple_tv_plus \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "63 /browse/tv_series_browse/affiliates:amazon_prime \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "64 /browse/tv_series_browse/sort:popular \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "65 /tv/shelter/s01 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "66 https://rottentomatoes.com/daily \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "67 https://editorial.rottentomatoes.com/ \n",
      " Movie: News\n",
      "Movie info: N/A\n",
      "68 https://editorial.rottentomatoes.com/all-time-lists/ \n",
      " Movie: All-Time Lists\n",
      "Movie info: N/A\n",
      "69 https://editorial.rottentomatoes.com/binge-guide/ \n",
      " Movie: Binge Guide\n",
      "Movie info: N/A\n",
      "70 https://editorial.rottentomatoes.com/comics-on-tv/ \n",
      " Movie: Comics on TV\n",
      "Movie info: N/A\n",
      "71 https://editorial.rottentomatoes.com/countdown/ \n",
      " Movie: Countdown\n",
      "Movie info: N/A\n",
      "72 https://editorial.rottentomatoes.com/five-favorite-films/ \n",
      " Movie: Five Favorite Films\n",
      "Movie info: N/A\n",
      "73 https://editorial.rottentomatoes.com/video-interviews/ \n",
      " Movie: Video Interviews\n",
      "Movie info: N/A\n",
      "74 https://editorial.rottentomatoes.com/weekend-box-office/ \n",
      " Movie: Weekend Box Office\n",
      "Movie info: N/A\n",
      "75 https://editorial.rottentomatoes.com/weekly-ketchup/ \n",
      " Movie: Weekly Ketchup\n",
      "Movie info: N/A\n",
      "76 https://editorial.rottentomatoes.com/what-to-watch/ \n",
      " Movie: What to Watch\n",
      "Movie info: N/A\n",
      "77 https://editorial.rottentomatoes.com/guide/video-game-movies/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "78 https://editorial.rottentomatoes.com/guide/star-wars-tv-ranked/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "79 https://editorial.rottentomatoes.com/countdown/ \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "80 https://editorial.rottentomatoes.com/rt-hub/rt25/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "81 https://editorial.rottentomatoes.com/rt-hub/what-to-watch/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "82 https://editorial.rottentomatoes.com/rt-hubs/ \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "83 https://editorial.rottentomatoes.com/article/9-must-watch-films-at-the-venice-international-film-festival/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "84 https://editorial.rottentomatoes.com/article/renewed-and-cancelled-tv-shows-2023/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "85 https://editorial.rottentomatoes.com/news/ \n",
      " Movie: View All\n",
      "Movie info: N/A\n",
      "86 https://www.fandango.com/movies-in-theaters \n",
      " Movie: Showtimes\n",
      "Movie info: N/A\n",
      "87 fandan.co/CinemaDay2023 \n",
      " Movie: National Cinema Day\n",
      "Movie info: N/A\n",
      "88 https://www.rottentomatoes.com/tv/star_wars_ahsoka/s01 \n",
      " Movie: Ahsoka\n",
      "Movie info: N/A\n",
      "89 https://www.rottentomatoes.com/m/blue_beetle \n",
      " Movie: Blue Beetle\n",
      "Movie info: N/A\n",
      "90 https://www.rottentomatoes.com/m/past_lives \n",
      " Movie: Past Lives\n",
      "Movie info: N/A\n",
      "91 https://www.rottentomatoes.com/m/bottoms \n",
      " Movie: Bottoms\n",
      "Movie info: N/A\n",
      "92 https://www.facebook.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "93 https://twitter.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "94 https://www.instagram.com/rottentomatoes/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "95 https://www.pinterest.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "96 https://www.youtube.com/user/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "97 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "98 /browse/movies_in_theaters \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "99 /browse/movies_at_home \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "100 /browse/movies_coming_soon \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "101 /browse/tv_series_browse \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "102 /browse/movies_in_theaters \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "103 /browse/movies_at_home \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "104 /browse/movies_coming_soon \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "105 /browse/tv_series_browse \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "106 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "107 # \n",
      " Movie: Clear all\n",
      "Movie info: N/A\n",
      "108 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "109 # \n",
      " Movie: Clear all\n",
      "Movie info: N/A\n",
      "110 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "111 # \n",
      " Movie: Clear all\n",
      "Movie info: N/A\n",
      "112 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "113 # \n",
      " Movie: Clear all\n",
      "Movie info: N/A\n",
      "114 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "115 # \n",
      " Movie: Clear all\n",
      "Movie info: N/A\n",
      "116 # \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "117 /m/past_lives \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "118 /m/how_to_blow_up_a_pipeline \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "119 /m/the_night_of_the_12th \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "120 /m/elemental_2023 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "121 /m/red_white_and_royal_blue \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "122 /m/asteroid_city \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "123 /m/spider_man_across_the_spider_verse \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "124 /m/jurassic_park \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "125 /m/they_cloned_tyrone \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "126 /m/the_stranger_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "127 /m/guardians_of_the_galaxy_vol_3 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "128 /m/dungeons_and_dragons_honor_among_thieves \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "129 /m/prey_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "130 /m/joy_ride_2023 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "131 /m/guy_ritchies_the_covenant \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "132 /m/oldboy \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "133 /m/blackberry \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "134 /m/the_blackening \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "135 /m/barbarian_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "136 /m/huesera_the_bone_woman \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "137 /m/sisu_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "138 /m/fresh_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "139 /m/interstellar_2014 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "140 /m/are_you_there_god_its_me_margaret \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "141 /m/m3gan \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "142 /m/influencer \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "143 /m/you_hurt_my_feelings_2023 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "144 /m/fire_island_2022 \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "145 /help_desk \n",
      " Movie: Help\n",
      "Movie info: N/A\n",
      "146 /about \n",
      " Movie: About Rotten Tomatoes\n",
      "Movie info: N/A\n",
      "147 /about#whatisthetomatometer \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "148 /critics/criteria \n",
      " Movie: Critic Submission\n",
      "Movie info: N/A\n",
      "149 /help_desk/licensing \n",
      " Movie: Licensing\n",
      "Movie info: N/A\n",
      "150 https://together.nbcuni.com/advertise/?utm_source=rotten_tomatoes&utm_medium=referral&utm_campaign=property_ad_pages&utm_content=footer \n",
      " Movie: Advertise With Us\n",
      "Movie info: N/A\n",
      "151 //www.fandango.com/careers \n",
      " Movie: Careers\n",
      "Movie info: N/A\n",
      "152 https://www.facebook.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "153 https://twitter.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "154 https://www.instagram.com/rottentomatoes/ \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "155 https://www.pinterest.com/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "156 https://www.youtube.com/user/rottentomatoes \n",
      " Movie: N/A\n",
      "Movie info: N/A\n",
      "157 //www.fandango.com/policies/privacy-policy \n",
      " Movie: Privacy Policy\n",
      "Movie info: N/A\n",
      "158 //www.fandango.com/policies/terms-and-policies \n",
      " Movie: Terms and Policies\n",
      "Movie info: N/A\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'javascript:void(0)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2540\\2965180615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmovie_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mmovie_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mmovie_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\SRMIST\\Python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\SRMIST\\Python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\SRMIST\\Python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\SRMIST\\Python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# Get the appropriate adapter to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m         \u001b[0madapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;31m# Start time (approximately) of the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\SRMIST\\Python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;31m# Nothing matches :-/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No connection adapters were found for {url!r}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for 'javascript:void(0)'"
     ]
    }
   ],
   "source": [
    "#Program-3 for Web crawling\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xlwt\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "base_url = 'https://www.rottentomatoes.com/top/bestofrt/'\n",
    "headers = {'User-Agent': 'Chrome/109.0.5414.149'}  # Add your user agent\n",
    "\n",
    "response = requests.get(base_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# Find all anchor (a) tags in the page\n",
    "anchor_tags = soup.find_all('a')\n",
    "\n",
    "line = 0\n",
    "num = 1\n",
    "\n",
    "# Create a new Excel workbook and add a worksheet\n",
    "workbook = xlwt.Workbook(encoding='utf-8')\n",
    "worksheet = workbook.add_sheet('Movie Data')\n",
    "\n",
    "for anchor in anchor_tags:\n",
    "    href = anchor.get('href')\n",
    "    if href:\n",
    "        movie_url = urljoin(base_url, href)\n",
    "        movie_f = requests.get(movie_url, headers=headers)\n",
    "        movie_soup = BeautifulSoup(movie_f.content, 'lxml')\n",
    "        \n",
    "        movie_content = movie_soup.find('div', {'class': 'movie_synopsis clamp clamp-6 js-clamp'})\n",
    "        if anchor.string:\n",
    "            movie_name = anchor.string.strip()\n",
    "        else:\n",
    "            movie_name = \"N/A\"\n",
    "        \n",
    "        if movie_content:\n",
    "            movie_info = movie_content.get_text().strip()\n",
    "        else:\n",
    "            movie_info = \"N/A\"\n",
    "        \n",
    "        print(num, href, '\\n', 'Movie:', movie_name)\n",
    "        print('Movie info:', movie_info)\n",
    "        \n",
    "        # Write data to the Excel worksheet\n",
    "        worksheet.write(line, 0, num)\n",
    "        worksheet.write(line, 1, href)\n",
    "        worksheet.write(line, 2, movie_name)\n",
    "        worksheet.write(line, 3, movie_info)\n",
    "        \n",
    "        line += 1\n",
    "        num += 1\n",
    "\n",
    "# Save the Excel workbook\n",
    "workbook.save('movies_top100.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f867372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website found: https://www.google\n",
      "Website found: https://maps.google\n",
      "Website found: https://play.google\n",
      "Website found: https://www.youtube\n",
      "Website found: https://news.google\n",
      "Website found: https://mail.google\n",
      "Website found: https://drive.google\n",
      "Website found: https://accounts.google\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "[WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "<urlopen error [Errno 11002] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "#Program-4 for Web crawling\n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import deque\n",
    "\n",
    "class WebCrawler:\n",
    "    def __init__(self):\n",
    "        self.queue = deque()\n",
    "        self.discovered_websites = set()\n",
    "\n",
    "    def discover(self, root):\n",
    "        self.queue.append(root)\n",
    "        self.discovered_websites.add(root)\n",
    "        \n",
    "        while self.queue:\n",
    "            v = self.queue.popleft()\n",
    "            raw = self.read_url(v)\n",
    "            regex = r\"https://(\\w+\\.)?(\\w+)\"\n",
    "            pattern = re.compile(regex)\n",
    "            matches = pattern.finditer(raw)\n",
    "            \n",
    "            for match in matches:\n",
    "                actual = match.group()\n",
    "                if actual not in self.discovered_websites:\n",
    "                    self.discovered_websites.add(actual)\n",
    "                    print(\"Website found:\", actual)\n",
    "                    self.queue.append(actual)\n",
    "\n",
    "    def read_url(self, v):\n",
    "        raw = \"\"\n",
    "        try:\n",
    "            response = urllib.request.urlopen(v)\n",
    "            data = response.read()\n",
    "            raw = data.decode(\"utf-8\")\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        return raw\n",
    "\n",
    "def main():\n",
    "    web_crawler = WebCrawler()\n",
    "    root = \"https://www.google.com\"\n",
    "    web_crawler.discover(root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bdedb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
